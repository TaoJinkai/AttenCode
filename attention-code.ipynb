{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:07:00.346116Z","iopub.execute_input":"2025-02-19T10:07:00.346385Z","iopub.status.idle":"2025-02-19T10:07:00.639630Z","shell.execute_reply.started":"2025-02-19T10:07:00.346366Z","shell.execute_reply":"2025-02-19T10:07:00.638791Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# %pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:07:00.640854Z","iopub.execute_input":"2025-02-19T10:07:00.641270Z","iopub.status.idle":"2025-02-19T10:07:00.644663Z","shell.execute_reply.started":"2025-02-19T10:07:00.641238Z","shell.execute_reply":"2025-02-19T10:07:00.643885Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import wandb\nimport os\nWANDB_API_KEY = 'your_api_key'\nwandb.login(\n    key=WANDB_API_KEY\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:07:00.645948Z","iopub.execute_input":"2025-02-19T10:07:00.646260Z","iopub.status.idle":"2025-02-19T10:07:08.936661Z","shell.execute_reply.started":"2025-02-19T10:07:00.646232Z","shell.execute_reply":"2025-02-19T10:07:08.936017Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjinkaitao-comm\u001b[0m (\u001b[33mjinkaitao-comm-central-university-of-finance-and-economics\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, AutoTokenizer\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n# 加载 wmt17 数据集（中文-英文）\nds = load_dataset(\"wmt/wmt17\", \"zh-en\")\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nvocab_size = len(tokenizer.vocab)\n# 'bert-base-uncased' vocab_size:\n\n# model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n# vocab_size \n# vocab_size = tokenizer.vocab_size\n# print(f\"vocab size:{vocab_size}\")\n\nclass TranslationDataset(Dataset):\n    def __init__(self, dataset, tokenizer, max_length=128):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        source = self.dataset[idx]['translation']['zh']\n        target = self.dataset[idx]['translation']['en']\n        \n        # Tokenization\n        source_encodings = self.tokenizer(source, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        target_encodings = self.tokenizer(target, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n                        \n        # We want to return input_ids and attention_mask for both source and target\n        return {\n            'source_text': source,  # original source text\n            'target_text': target,  # original target text\n            'input_ids': source_encodings['input_ids'].squeeze(0),  # Remove the batch dimension\n            'attention_mask': source_encodings['attention_mask'].squeeze(0),\n            'labels': target_encodings['input_ids'].squeeze(0)  # Labels are the target sequence\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:07:08.937855Z","iopub.execute_input":"2025-02-19T10:07:08.938239Z","iopub.status.idle":"2025-02-19T10:08:12.536578Z","shell.execute_reply.started":"2025-02-19T10:07:08.938217Z","shell.execute_reply":"2025-02-19T10:08:12.535718Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83953179c2614540bf287085bd7b3e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00013.parquet:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9743c62c6b644c6f898e17d64db1116d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00013.parquet:   0%|          | 0.00/272M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a079667162d6442bae139bd244d4535c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00013.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad8dda32faf445a689ec49c205a7dd81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00013.parquet:   0%|          | 0.00/278M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8d2625d2394380973f9f8fd4b30b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00013.parquet:   0%|          | 0.00/277M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27dffc56392436a85c7f3092251a43f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00013.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b1506b178f478a8a72c6c7b1ea980e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00013.parquet:   0%|          | 0.00/282M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e853bf0da81e48da89e8684e06f2214a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00013.parquet:   0%|          | 0.00/281M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845251c908a54f2383995415b541c4de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00008-of-00013.parquet:   0%|          | 0.00/294M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"250287ae7277493d85db481c29df84fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00009-of-00013.parquet:   0%|          | 0.00/272M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42e45990e110443e9b219bcef581eb57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00010-of-00013.parquet:   0%|          | 0.00/191M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45956b819e90462eb33bbbdfb0f98860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00011-of-00013.parquet:   0%|          | 0.00/327M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feabb3f545984fd79c9f4a1847d37b91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00012-of-00013.parquet:   0%|          | 0.00/254M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54cd75eb29d4c34801621bc2c291ab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/394k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e19964aaf82d493990dcb946e59c5e24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/362k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe6e358f03745ff8d8006b9ed55a995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25134743 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7728f9ed9e7f477683ac55fca7d0178b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2002 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75840d06c2c149329e2170111c35e63e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7022f32c8714a5bbf65f0dd343885e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1a9d39bf12444d09be16ba3af24b620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533871d13c53450d95c2ac1c2ea59dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae44f71bf434428a9d3927d10e3b15b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51157d4725dc466394e0c956bd2f0772"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler\n\ndef get_dataloader(dataset, tokenizer, batch_size, is_train=True, max_length=128, num_samples=None):\n    if is_train:\n        full_dataset = dataset['train']\n    else:\n        full_dataset = dataset['validation']  \n\n    sub_dataset = TranslationDataset(full_dataset, tokenizer, max_length)\n\n    if num_samples is not None:\n        sampler = RandomSampler(sub_dataset, replacement=False, num_samples=num_samples)\n    else:\n        sampler = RandomSampler(sub_dataset) if is_train else None\n\n    loader = DataLoader(\n        dataset=sub_dataset,\n        batch_size=batch_size,\n        shuffle=False,  # 不需要shuffle，因为sampler已经处理了\n        pin_memory=True,\n        num_workers=2,\n        sampler=sampler\n    )\n    return loader\n\n# test DataLoader\ntestdata_loader = get_dataloader(ds, tokenizer, batch_size=2, is_train=True, num_samples=3)\n# testdata_loader = get_dataloader(ds, tokenizer, batch_size=2, is_train=False, num_samples=3)\nfor batch in testdata_loader:\n    source_texts = batch['source_text']  # original source text\n    target_texts = batch['target_text']  # original target text\n    src = batch['input_ids']\n    trg = batch['labels']\n    trg_input = trg[:, :-1]\n    target = trg[:, 1:]\n    # Print original texts\n    print(source_texts[0])\n    print(target_texts[0])\n    # Print the shapes of the tensors\n    print(batch.keys())\n    print(f\"src shape: {src.shape}\")\n    print(f\"trg shape: {trg.shape}\")\n    print(f\"trg_input shape: {trg_input.shape}\")\n    print(f\"target shape: {target.shape}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:08:12.537488Z","iopub.execute_input":"2025-02-19T10:08:12.538049Z","iopub.status.idle":"2025-02-19T10:08:15.982682Z","shell.execute_reply.started":"2025-02-19T10:08:12.538025Z","shell.execute_reply":"2025-02-19T10:08:15.981687Z"}},"outputs":[{"name":"stdout","text":"友谊常以爱情而告终，而爱情常不能以友谊而结束。\nFriendship often ends in love, but love in friendship, never.\ndict_keys(['source_text', 'target_text', 'input_ids', 'attention_mask', 'labels'])\nsrc shape: torch.Size([2, 128])\ntrg shape: torch.Size([2, 128])\ntrg_input shape: torch.Size([2, 127])\ntarget shape: torch.Size([2, 127])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 查看训练集的前几条数据\nprint(ds['train'][:2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:08:15.983873Z","iopub.execute_input":"2025-02-19T10:08:15.984213Z","iopub.status.idle":"2025-02-19T10:08:17.887034Z","shell.execute_reply.started":"2025-02-19T10:08:15.984176Z","shell.execute_reply":"2025-02-19T10:08:17.886026Z"}},"outputs":[{"name":"stdout","text":"{'translation': [{'en': '1929 or 1989?', 'zh': '1929年还是1989年?'}, {'en': 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.', 'zh': '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。'}]}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Build the model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm import tqdm\nimport math\n\n# PE\nclass PositionalEncoder(nn.Module):\n    def __init__(self, d_model,max_seq_len=128):\n        super().__init__()\n        self.d_model = d_model # demension of model\n        pe = torch.zeros(max_seq_len,d_model)\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = math.sin(pos / (10000**((2 * i) / d_model)))\n                pe[pos, i + 1] = math.cos(pos / (10000**((2 * (i + 1)) / d_model)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self,x):\n        x = x*math.sqrt(self.d_model)\n        seq_len = x.size(1)\n        x = x+self.pe[:,:seq_len]\n        return x\n\n# MHA\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, heads, d_model, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // heads\n        self.h = heads\n        \n        # Linear projections for Q, K, V\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        \n        # Dropout and final linear projection\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n\n    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:            \n            # Ensure mask has the correct shape (batch_size, 1, seq_len) or (batch_size, 1, 1, seq_len)\n            if len(mask.shape) == 3:  # (batch_size, 1, seq_len)\n                mask = mask.unsqueeze(1)  # Add a dimension for heads: (batch_size, 1, 1, seq_len)\n            elif len(mask.shape) == 4:  # (batch_size, 1, 1, seq_len)\n                pass  # Already in the correct shape\n            else:\n                raise ValueError(f\"Unexpected mask shape {mask.shape}\")\n            \n            mask = mask.repeat(1, self.h, 1, 1)  # Repeat for each head: (batch_size, heads, 1, seq_len)\n            scores = scores.masked_fill(mask == 0, -1e9)\n        scores = F.softmax(scores, dim=-1)\n        if dropout is not None:\n            scores = dropout(scores)\n        output = torch.matmul(scores, v)\n        return output\n\n    def forward(self, q, k, v, mask=None):\n        bs = q.size(0)\n        # Linear projections and split into heads\n        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)  # (bs, h, seq_len, d_k)\n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)  # (bs, h, seq_len, d_k)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)  # (bs, h, seq_len, d_k)\n        \n        # Apply attention mechanism\n        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n        \n        # Concatenate heads and project back to original dimension\n        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n        \n        output = self.out(concat)\n        return output\n\n\n# FF\nclass FeedForward(nn.Module):\n\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        # d_ff default is 2048\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear_1(x)))\n        x = self.linear_2(x)\n        return x\n\n\n# Norm\nclass NormLayer(nn.Module):\n\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n        self.size = d_model\n        # two learnable parameters\n        self.alpha = nn.Parameter(torch.ones(self.size))\n        self.bias = nn.Parameter(torch.zeros(self.size))\n        self.eps = eps\n\n    def forward(self, x):\n        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n        return norm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:08:17.888114Z","iopub.execute_input":"2025-02-19T10:08:17.888440Z","iopub.status.idle":"2025-02-19T10:08:31.464242Z","shell.execute_reply.started":"2025-02-19T10:08:17.888408Z","shell.execute_reply":"2025-02-19T10:08:31.463585Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Local Attention","metadata":{}},{"cell_type":"code","source":"class LocalAttention(nn.Module):\n    def __init__(self, heads, d_model, dropout=0.1,window_size=64):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // heads\n        self.h = heads\n        self.window_size = window_size\n        \n        # Linear projections for Q, K, V\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        \n        # Dropout and final linear projection\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n\n    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n        # Apply local attention by restricting the attention scores to a local window\n        batch_size, heads, seq_len, _ = q.size()\n        # print(q.size())\n        \n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        # print(f\"scores shape ：{scores.shape}\")\n        \n        # Limit attention to local window\n        window_mask = self.create_local_mask(scores, seq_len).to(q.device)\n        # print(f\"window mask shape：{window_mask.shape}\")\n        scores = scores * window_mask\n        \n        \n        if mask is not None:            \n            # Ensure mask has the correct shape (batch_size, 1, seq_len) or (batch_size, 1, 1, seq_len)\n            if len(mask.shape) == 3:  # (batch_size, 1, seq_len)\n                mask = mask.unsqueeze(1)  # Add a dimension for heads: (batch_size, 1, 1, seq_len)\n            elif len(mask.shape) == 4:  # (batch_size, 1, 1, seq_len)\n                pass  # Already in the correct shape\n            else:\n                raise ValueError(f\"Unexpected mask shape {mask.shape}\")\n            \n            mask = mask.repeat(1, self.h, 1, 1)  # Repeat for each head: (batch_size, heads, 1, seq_len)\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        scores = F.softmax(scores, dim=-1)\n        \n        if dropout is not None:\n            scores = dropout(scores)\n        output = torch.matmul(scores, v)\n        return output\n\n\n    def create_local_mask(self, scores, seq_len):\n        batch_size, num_heads, seq_len_x, seq_len_y = scores.shape  # 从scores中获取实际的seq_len_x和seq_len_y\n        mask = torch.ones(seq_len_x, seq_len_y).tril(diagonal=self.window_size)  # 创建局部窗口掩码\n        mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len_x, seq_len_y)\n        mask = mask.repeat(batch_size, num_heads, 1, 1)  # (batch_size, heads, seq_len_x, seq_len_y)\n    \n        return mask\n\n    def forward(self, q, k, v, mask=None):\n        bs = q.size(0)\n        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n        \n        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n        \n        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n        \n        output = self.out(concat)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:08:31.466324Z","iopub.execute_input":"2025-02-19T10:08:31.466874Z","iopub.status.idle":"2025-02-19T10:08:31.476531Z","shell.execute_reply.started":"2025-02-19T10:08:31.466848Z","shell.execute_reply":"2025-02-19T10:08:31.475624Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Spare Attention","metadata":{}},{"cell_type":"code","source":"class SparseAttention(nn.Module):\n    def __init__(self, heads, d_model, sparsity=0.1, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // heads\n        self.h = heads\n        self.sparsity = sparsity\n        \n        # Linear projections for Q, K, V\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        \n        # Dropout and final linear projection\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n\n    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        \n        # Sparse attention: Mask out some parts of the attention matrix\n        sparse_mask = torch.rand_like(scores) > self.sparsity\n        scores = scores.masked_fill(sparse_mask == 0, -1e9)\n        \n        if mask is not None:            \n            # Ensure mask has the correct shape (batch_size, 1, seq_len) or (batch_size, 1, 1, seq_len)\n            if len(mask.shape) == 3:  # (batch_size, 1, seq_len)\n                mask = mask.unsqueeze(1)  # Add a dimension for heads: (batch_size, 1, 1, seq_len)\n            elif len(mask.shape) == 4:  # (batch_size, 1, 1, seq_len)\n                pass  # Already in the correct shape\n            else:\n                raise ValueError(f\"Unexpected mask shape {mask.shape}\")\n            \n            mask = mask.repeat(1, self.h, 1, 1)  # Repeat for each head: (batch_size, heads, 1, seq_len)\n            scores = scores.masked_fill(mask == 0, -1e9)\n                    \n        scores = F.softmax(scores, dim=-1)\n        if dropout is not None:\n            scores = dropout(scores)\n        \n        output = torch.matmul(scores, v)\n        return output\n\n    def forward(self, q, k, v, mask=None):\n        bs = q.size(0)\n        q = self.q_linear(q).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k).transpose(1, 2)\n        \n        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n        \n        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n        \n        output = self.out(concat)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:08:31.478120Z","iopub.execute_input":"2025-02-19T10:08:31.478421Z","iopub.status.idle":"2025-02-19T10:08:31.510451Z","shell.execute_reply.started":"2025-02-19T10:08:31.478391Z","shell.execute_reply":"2025-02-19T10:08:31.509769Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Convolutional Attention","metadata":{}},{"cell_type":"code","source":"class ConvolutionalAttention(nn.Module):\n    def __init__(self, heads, d_model, kernel_size=3, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_k = d_model // heads\n        self.h = heads\n\n        # Convolutional projections for Q, K, V\n        self.q_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2)\n        self.k_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2)\n        self.v_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size//2)\n\n        # Dropout and final linear projection\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n\n    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n        # Scaled dot-product attention\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            if len(mask.shape) == 3:\n                mask = mask.unsqueeze(1)\n            elif len(mask.shape) == 4:\n                pass\n            else:\n                raise ValueError(f\"Unexpected mask shape {mask.shape}\")\n            mask = mask.repeat(1, self.h, 1, 1)\n            scores = scores.masked_fill(mask == 0, -1e9)\n        scores = F.softmax(scores, dim=-1)\n        if dropout is not None:\n            scores = dropout(scores)\n        output = torch.matmul(scores, v)\n        return output\n\n    def forward(self, q, k, v, mask=None):\n        bs, seq_len, d_model = q.size()\n\n        # Apply convolutional projections\n        q = self.q_conv(q.transpose(1, 2)).transpose(1, 2)\n        k = self.k_conv(k.transpose(1, 2)).transpose(1, 2)\n        v = self.v_conv(v.transpose(1, 2)).transpose(1, 2)\n\n        # Split into heads and adjust shape\n        q = q.view(bs, seq_len, self.h, self.d_k).transpose(1, 2)  # (bs, h, seq_len, d_k)\n        k = k.view(bs, seq_len, self.h, self.d_k).transpose(1, 2)  # (bs, h, seq_len, d_k)\n        v = v.view(bs, seq_len, self.h, self.d_k).transpose(1, 2)  # (bs, h, seq_len, d_k)\n\n        # Apply attention mechanism\n        scores = self.attention(q, k, v, self.d_k, mask, self.dropout)\n\n        # Concatenate heads and project back to original dimension\n        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n        output = self.out(concat)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:42.191635Z","iopub.execute_input":"2025-02-19T10:20:42.191973Z","iopub.status.idle":"2025-02-19T10:20:42.202518Z","shell.execute_reply.started":"2025-02-19T10:20:42.191946Z","shell.execute_reply":"2025-02-19T10:20:42.201807Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Encoder text -> vector\nclass EncoderLayer(nn.Module):\n\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = NormLayer(d_model)\n        self.norm_2 = NormLayer(d_model)\n        # self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n        self.attn = ConvolutionalAttention(heads, d_model, dropout=dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_output = self.attn(x, x, x, mask)\n        x = x + self.dropout_1(attn_output) \n        \n        x = self.norm_1(x)  \n\n        ff_output = self.ff(x) \n        x = x + self.dropout_2(ff_output)  \n        x = self.norm_2(x)  \n\n        return x\n\n\nclass Encoder(nn.Module):\n\n    def __init__(self, vocab_size, d_model, N, heads, dropout):\n        super().__init__()\n        self.N = N\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pe = PositionalEncoder(d_model)\n        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, dropout) for _ in range(N)])\n        self.norm = NormLayer(d_model)\n\n    def forward(self, src, mask):\n        x = self.embed(src)\n        x = self.pe(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\n    \n# Decoder\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = NormLayer(d_model)\n        self.norm_2 = NormLayer(d_model)\n        self.norm_3 = NormLayer(d_model)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n        self.attn_1 = ConvolutionalAttention(heads, d_model, dropout=dropout)\n        self.attn_2 = ConvolutionalAttention(heads, d_model, dropout=dropout)\n        # self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n        # self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n\n    def forward(self, x, e_outputs, src_mask, trg_mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n        x2 = self.norm_3(x)\n        x = x + self.dropout_3(self.ff(x2))\n        return x\n\nclass Decoder(nn.Module):\n\n    def __init__(self, vocab_size, d_model, N, heads, dropout):\n        super().__init__()\n        self.N = N\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pe = PositionalEncoder(d_model)\n        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, dropout) for _ in range(N)])\n        self.norm = NormLayer(d_model)\n\n    def forward(self, trg, e_outputs, src_mask, trg_mask):\n        x = self.embed(trg)\n        x = self.pe(x)\n        for layer in self.layers:\n            x = layer(x, e_outputs, src_mask, trg_mask)\n        return self.norm(x)\n\nclass Transformer(nn.Module):\n\n    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n        super().__init__()\n        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n        self.out = nn.Linear(d_model, trg_vocab)\n\n    def forward(self, src, trg, src_mask, trg_mask):\n        e_outputs = self.encoder(src, src_mask)\n        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n        output = self.out(d_output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:42.203743Z","iopub.execute_input":"2025-02-19T10:20:42.204026Z","iopub.status.idle":"2025-02-19T10:20:42.223486Z","shell.execute_reply.started":"2025-02-19T10:20:42.203998Z","shell.execute_reply":"2025-02-19T10:20:42.222647Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### Pre-process data\nSome data processing functions","metadata":{}},{"cell_type":"code","source":"def prepare_batch_for_transformer(batch, tokenizer, device):\n    \"\"\"\n    Prepare a batch for Transformer model during training.\n    This function will:\n    - Generate source and target masks.\n    - Shift target sequence for input (trg_input) and labels (target).\n    \n    Parameters:\n    - batch (dict): Batch of data containing 'input_ids' (source) and 'labels' (target).\n    - tokenizer: Tokenizer used to decode/encode tokens (e.g., for padding token ID).\n    - device: The device (CPU/GPU) on which to move the tensors.\n    \n    Returns:\n    - src (Tensor): Source input tensor.\n    - trg_input (Tensor): Target input tensor for the decoder.\n    - target (Tensor): Ground truth labels for loss calculation.\n    - src_mask (Tensor): Source mask to avoid attention to padding tokens.\n    - trg_mask (Tensor): Target mask to avoid looking ahead at future tokens.\n    \"\"\"\n    # Extract input and labels from the batch\n\n    src = batch['input_ids'].to(device)\n    trg = batch['labels'].to(device)\n    att_mask = batch['attention_mask'].to(device)\n\n    src = src & att_mask\n\n    # print(f\"att_mask:{att_mask.shape}\")\n\n    # Prepare the target inputs for the decoder (shifted left by 1)\n    trg_input = trg[:, :-1].to(device)  # Use all tokens except the last token of the target sequence\n    target = trg[:, 1:].to(device)      # Ground truth labels (shifted right by 1 for loss calculation)\n\n    # Generate masks\n    # src_mask = create_padding_mask(src).to(device) & att_mask.to(device)\n    src_mask = create_padding_mask(src).to(device)\n    trg_mask = create_padding_mask(trg_input) & create_subsequent_mask(trg_input.size(-1)).type_as(src_mask.data).to(device)\n\n    \n    return src, trg_input, target, src_mask, trg_mask\n\n# Function to create padding mask\ndef create_padding_mask(seq):\n    \"\"\"\n    Generates a mask to hide padding tokens in the input sequences.\n    \n    Parameters:\n    seq (torch.Tensor): Input sequence tensor of shape (batch_size, seq_length).\n    \n    Returns:\n    torch.Tensor: A mask tensor of shape (batch_size, 1, 1, seq_length) where '0' indicates positions that should be masked.\n    \"\"\"\n    # The condition (seq != 0) creates a boolean mask where padding tokens are False\n    mask = (seq != 0).unsqueeze(1).unsqueeze(2)\n    return mask\n\n# Function to create subsequent mask for decoder\ndef create_subsequent_mask(size):\n    \"\"\"\n    Generates a mask to prevent positions from attending to subsequent positions,\n    ensuring that each position only attends to previous and current positions.\n    \n    Parameters:\n    size (int): The length of the target sequence.\n    \n    Returns:\n    torch.Tensor: A lower triangular mask tensor of shape (1, size, size), where '1' indicates visible positions.\n    \"\"\"\n    # Create a tensor of ones with shape (1, size, size)\n    attn_shape = (1, size, size)\n    # Use torch.triu to get the upper triangular part of the matrix starting from diagonal=1, then invert it\n    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n    return subsequent_mask == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:42.224647Z","iopub.execute_input":"2025-02-19T10:20:42.224879Z","iopub.status.idle":"2025-02-19T10:20:42.238929Z","shell.execute_reply.started":"2025-02-19T10:20:42.224859Z","shell.execute_reply":"2025-02-19T10:20:42.238074Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def log_translation_table(source_texts, groundtruths, predicted, tokenizer):\n    \"Log a wandb.Table with (source, groundtruth, predicted)\"\n    table = wandb.Table(columns=[\"source\", \"groundtruth\", \"predicted\"])\n    \n    # Convert the source, groundtruth, and predicted tensors to text\n    predicted = predicted.to(\"cpu\")\n    \n    for src, gro, pred in zip(source_texts, groundtruths, predicted):\n        pre = tokenizer.decode(pred, skip_special_tokens=True)\n        table.add_data(src, gro, pre)\n    \n    wandb.log({\"translation_table\": table}, commit=False)\n\n\nfrom nltk.translate.bleu_score import sentence_bleu\n\ndef validate_model(model, valid_dl, loss_func, tokenizer, device, batch_idx=0):\n    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n    model.eval()\n    val_loss = 0.\n    total_bleu = 0.  # 用于计算 BLEU 分数\n    num_batches = 0  # 用于统计批次数\n    with torch.inference_mode():\n        for i, batch in enumerate(valid_dl):\n            # Get original source text\n            source_texts = batch['source_text']  # original source text\n            target_texts = batch['target_text']  # original target text\n            src, trg_input, target, src_mask, trg_mask = prepare_batch_for_transformer(batch, tokenizer, device)\n    \n            # Generate masks\n            src_mask = create_padding_mask(src).to(device)\n            trg_mask = create_padding_mask(trg_input) & create_subsequent_mask(trg_input.size(-1)).type_as(src_mask.data).to(device)\n\n\n            # Forward pass ➡\n            output = model(src, trg_input, src_mask, trg_mask)\n            # output = model(src, target_ids[:, :-1], src_mask=src_mask, trg_mask=trg_mask)\n            \n            # Calculate loss\n            val_loss += loss_func(output.reshape(-1, vocab_size), target.reshape(-1)).item() * target.size(0)\n\n            # Get the predicted output sequence (indices of max logit values)\n            _, predicted = torch.max(output, dim=-1)\n\n            # Calculate BLEU score for this batch\n            for idx in range(len(predicted)):\n                predicted_text = tokenizer.decode(predicted[idx], skip_special_tokens=True)\n                target_text = target_texts[idx]\n                \n                # BLEU score calculation (single reference, single hypothesis)\n                bleu_score = sentence_bleu([target_text.split()], predicted_text.split())\n                total_bleu += bleu_score\n            \n            # Log one batch of predictions to the dashboard, always same batch_idx.\n            if i == batch_idx:\n                log_translation_table(source_texts, target_texts, predicted, tokenizer)\n\n            num_batches += 1\n\n    avg_loss = val_loss / len(valid_dl.dataset)\n    avg_bleu = total_bleu / (num_batches * valid_dl.batch_size)  # Normalizing BLEU score over the dataset\n\n    return avg_loss, avg_bleu  # Return loss and BLEU score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:42.342269Z","iopub.execute_input":"2025-02-19T10:20:42.342566Z","iopub.status.idle":"2025-02-19T10:20:42.352116Z","shell.execute_reply.started":"2025-02-19T10:20:42.342539Z","shell.execute_reply":"2025-02-19T10:20:42.351352Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Start training","metadata":{}},{"cell_type":"code","source":"# initialise a wandb run\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nRUN_NAME = \"transformer-attention-CA\"\nwandb.init(\n    project=\"attention-mechanism-code\",\n    job_type='train',\n    name=RUN_NAME,\n    config={\n        \"epochs\": 20,\n        \"batch_size\": 32,\n        \"lr\": 1e-3,\n        \"d_model\":512,\n        \"N\":6,\n        \"heads\":8, \n        \"dropout\":0.1\n        })\n\n# Copy your config\nconfig = wandb.config\nprint(config)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n\n# Get the data\n# 10k ~ 50k \ntrain_loader = get_dataloader(ds, tokenizer, batch_size=config.batch_size, is_train=True, num_samples=20000)\nvalid_loader = get_dataloader(ds, tokenizer, batch_size=2*config.batch_size, is_train=False, num_samples=2000)\n\nn_steps_per_epoch = math.ceil(len(train_loader.dataset) / config.batch_size)\n\nmodel = Transformer(vocab_size , \n                    vocab_size , \n                    config.d_model, \n                    config.N, \n                    config.heads, \n                    config.dropout\n                   ).to(device)\n\n# Make the loss and optimizer\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n\n# Training\nexample_ct = 0\nstep_ct = 0\ntorch.cuda.empty_cache()\nfor epoch in range(config.epochs):\n    model.train()\n    print(f\"Epoch {epoch+1}/{config.epochs} - Start training\")\n    for step, batch in enumerate(train_loader):\n        # Prepare data for Transformer model\n        src, trg_input, target, src_mask, trg_mask = prepare_batch_for_transformer(batch, tokenizer, device)\n        # print(f\"src shape:{src.shape}\")\n        # print(f\"trg_input shape:{trg_input.shape}\")\n        # print(f\"src_mask shape:{src_mask.shape}\")\n        # print(f\"trg_mask shape:{trg_mask.shape}\")\n\n        # Check shape\n        # if src_input_ids.shape[1] != max_length or trg_input.shape[1] != max_length - 1:\n        #     print(f\"Batch {step} contains invalid sample shapes: skipping this batch.\")\n        #     continue  # 跳过这个批次\n\n        # Forward pass\n        output = model(src, trg_input, src_mask, trg_mask)\n        \n        # Ensure the tensor is contiguous before reshaping\n        output = output.contiguous()        \n        # Use reshape instead of view to avoid memory layout issues\n        reshaped_output = output.reshape(-1, vocab_size)\n        reshaped_target = target.reshape(-1)\n        \n        # Ensure dimensions match for loss calculation\n        assert reshaped_output.shape[0] == reshaped_target.shape[0], \"Batch sizes do not match\"\n        \n        # Calculate loss\n        train_loss = loss_func(reshaped_output, reshaped_target)\n        \n        # print(f\"Train loss: {train_loss}\")\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n\n        example_ct += src.size(0) \n        metrics = {\"train/train_loss\": train_loss.item(),\n                   \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n                   \"train/example_ct\": example_ct}\n\n        if step + 1 < n_steps_per_epoch:\n            # Log train metrics to wandb\n            wandb.log(metrics)\n\n        step_ct += 1\n    print(f\"          - Start validating\")\n    val_loss, avg_bleu = validate_model(model, valid_loader, loss_func, tokenizer, device)\n\n    # Log train and validation metrics to wandb\n    val_metrics = {\"val/val_loss\": val_loss,\n                   \"val/val_avg_bleu\": avg_bleu}\n    wandb.log({**metrics, **val_metrics})\n\n    # Save the model checkpoint to wandb\n    torch.save(model, \"transformer_model.pt\")\n    wandb.log_model(\"./transformer_model.pt\", \"transformer_model\", aliases=[f\"epoch-{epoch+1}_dropout-{round(wandb.config.dropout, 4)}\"])\n\n    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, BLEU: {avg_bleu:.2f}\")\n\n# If you had a test set, this is how you could log it as a Summary metric\nwandb.summary['test_avg_bleu'] = 0.8\n\n# Close your wandb run\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Find bugs\nfor test cells","metadata":{}},{"cell_type":"code","source":"# test code here","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:47.981559Z","iopub.status.idle":"2025-02-19T10:20:47.981964Z","shell.execute_reply":"2025-02-19T10:20:47.981799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, AutoTokenizer\n\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# 'bert-base-uncased' vocab_size:30522\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n# vocab_size \"vocab_size\": 65001\nprint(tokenizer.vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:47.982572Z","iopub.status.idle":"2025-02-19T10:20:47.982848Z","shell.execute_reply":"2025-02-19T10:20:47.982714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoConfig\n\ntokenizer_config = AutoConfig.from_pretrained(model_checkpoint)\ntokenizer_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T10:20:47.983692Z","iopub.status.idle":"2025-02-19T10:20:47.983988Z","shell.execute_reply":"2025-02-19T10:20:47.983875Z"}},"outputs":[],"execution_count":null}]}